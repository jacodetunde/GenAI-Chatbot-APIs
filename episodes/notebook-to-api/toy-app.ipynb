{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variable\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 26\n"
     ]
    }
   ],
   "source": [
    "# Embed the documents\n",
    "#embedding models\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "#document source\n",
    "doc_source_path = 'knowledgebase/'\n",
    "\n",
    "# Chunking variables\n",
    "chunk_size = 1800\n",
    "chunk_overlap = 300\n",
    "\n",
    "# Document Loader\n",
    "loader = DirectoryLoader(\n",
    "        doc_source_path,\n",
    "        glob=\"**/*.txt\", # \"**/*.pdf\" if loading pdf\n",
    "        loader_cls=TextLoader,# PyMuPDFLoader if loading pdf\n",
    "    )\n",
    "documents = loader.load()\n",
    "\n",
    "#Splitting documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "# Setting up vectorstore\n",
    "vectorstore = Qdrant.from_documents(texts, embeddings,\n",
    "                                    location=\":memory:\",\n",
    "                                    collection_name=\"PMarca\",) # \"PDFCollection\" if loading pdf\n",
    "\n",
    "#Optional: Determine the length of the document loaded and vectorized\n",
    "num_documents = len(texts)\n",
    "print(f\"Total number of documents: {num_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate document retriever \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up caching system locally\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for retriever to work in LCEL chain fashion\n",
    "def retrieve(inputs):\n",
    "  return [doc.page_content for doc in retriever.invoke(inputs[\"question\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting LLM parameters\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7,max_tokens=800, top_p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll set our initial prompt - which will be initialize our LCEL chain.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given {context}, answer the question `{question}` as a bullet points. Most of your response should come from the {context}.\n",
    "      Be creative, concise and be as practical as possible\"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=retrieve) | prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court four days ago.\\n- He described her as one of the nation’s top legal minds who will continue Justice Stephen Breyer’s legacy of excellence.\\n- Ketanji Brown Jackson has a background as a former top litigator in private practice and a former federal public defender.\\n- She comes from a family of public school educators and police officers, emphasizing her community ties.\\n- The President highlighted her ability to build consensus and noted the broad range of support she has received from various organizations, including the Fraternal Order of Police and former judges from both political parties.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did the president say about ketanji brown?\"\n",
    "\n",
    "rag_chain.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
